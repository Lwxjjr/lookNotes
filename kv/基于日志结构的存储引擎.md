# 基于日志结构的存储引擎解决了什么问题？
为了解决==写多读少==的特定场景而提出的解决方案
因为在LSM出现之前，数据库索引几乎是 B-Tree，但是 B-Tree 在面临大量写入时效率回急剧下降，原因是：
1. 随机IO：原地更新的过程导致充斥着大量的随机磁盘读写，磁头的寻道时间是性能的主要瓶颈
2. 页分裂和合并，增加了I/O开销和锁竞争
> [!info] 常见的场景
> 1. 日志系统：需要记录大量前台和后台应用产生的大量日志，以便做服务监控，数据分析等问题
> 2. 物联网IoT数据采集：如智能手环的心率监测、共享单车的定位上报、智能电表的数据回传、工厂传感器的状态更新，时时刻刻向云端发送数据，可能只是为了形成一个总览报表
> 3. 推荐系统：时时刻刻记录需要记录用户的行为信息，用来训练线上运行的推荐模型

> [!example] 一个简单的比喻
> **读多写少就像图书馆**
> 写操作：图书管理员采购新书、上架。这个动作频率很低
> 读操作：成千上万的读者来借书、阅览。这个动作频率极高
> 挑战：如何让很多人能同时、快速地找到并阅读他们想要的书？（高并发读取）
> 
> **写多读少就像邮筒**
> 写操作：无数人随时随地往邮筒里投信。这个动作频率非常高
> 读操作：邮递员每天只在一两个固定时间来取信。这个动作频率很低
> 挑战：如何确保每一封信都能被投进来，不会丢失，邮筒不会被塞爆？（高吞吐量写入）

---
# LSM Tree（Log-structured Merge-Tree）
![[Pasted image 20250717223918.png]]
本质的核心思想--**通过将随机写转换为顺序写，来换取极高的写入性能**

<h3>看来一下从想法方面的推导：</h3>
为了解决随机IO的问题，==将所有随机写入操作全部转换成"顺序追加写入"==
当我们进行写操作，全部追加到一个文件中记录下来，通过 Type 来区分操作类型

![[Pasted image 20250718001101.png]]
因为是标记位置追加写，对象的删除相关的记录依然占据空间，显然空间利用率不高。
我们将该情景称为**空间放大(一条(有效or无效)记录占据了多份磁盘空间)**
因此需要做优化处理
==通过 Merge 操作堆数据文件进行清理，清理掉无用的数据==

但是单文件的 Merge 引入新问题，由于单文件数据膨胀，一方面压缩时会阻塞我们新的写入和读取操作，另一方面压缩需要读取整个文件遍历，较为耗时。
这里我们如果能够==数据分段存储==，就能很好的解决这个问题。当一个文件写入的数据达到一定条件后就关闭，重新打开一个文件进行写入。对前面已经关闭的文件进行定期的压缩，此时压缩就不会阻塞读写操作了

![[Pasted image 20250718004519.png]]

这样处理后，我们的压缩逻辑大致是：
1. 倒序读取多个文件到内存
2. 合并数据到新文件中

显然，依然存在压缩慢的问题
因为我们的合并依然是顺序读取标识合并，如果我们能多个文件同时合并的话速度能大大提升，Compaction的底层支持算法就是经典的 K-Way Merge Sort，只需要让我们归并的每个文件有序，压缩的效率就能大大提升，因此怎么在内存中写入文件时数据就排好序是个问题，也就是需要==一个支持排序的数据结构==
对此可选有 AVL、B-Tree、BWTree、SkipList 等
==数据压缩策略的经典选择有==：
- 分级压缩
- 分层压缩
![[Pasted image 20250718092324.png]]

引入了内存存储后，如果在缓存期间还没有来得及刷盘，程序异常后数据就丢失了，怎么办？那么在数据缓存到内存之前，先记录到日志中。当程序异常挂掉后重新启动时，就可以根据日志文件中记录来恢复数据，也就是 ==WAL==

到这里我们的模型有了一个很大的改变，引入了内存模型，WAL日志
下面我们对几个模块进行分层：
- 内存数据 Mem Table：主要缓存我们写入进来的数据
- 磁盘数据文件 SS Table：保存我们所有的用户数据，有序存储
- 磁盘日志文件 WAL log：预习日志文件，用于恢复文件

---
# WiscKey
